---
title: 'Mineria de dades: PRA2 - Projecte de mineria de dades'
author: "Autor: Ignasi Grau Muñoz"
date: "Gener 2024"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 05.584-PAC-header-4.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


******
# Enunciat
******

Com a continuació de l'estudi iniciat a la Pràctica 1, procedim a **aplicar models analítics, tant no supervisats com supervisats**, sobre el joc de dades seleccionat i preparat. En aquesta **Pràctica 2** haureu de carregar les dades prèviament preparades a la **Pràctica 1**.


**Punt comú per a tots els exercicis**

En tots els apartats dels exercicis d'aquesta pràctica es demana a l'estudiant, a més d'aplicar els diferents mètodes i d'analitzar correctament el problema, **detallar de manera exhaustiva** ressaltant el perquè i com s'ha realitzat, incloent-hi elements visuals, explicant els resultats i realitzant les comparatives oportunes amb les seves conclusions.

Per a tota la pràctica és necessari documentar cada apartat de l'exercici pràctic que s'ha fet, el perquè s'ha fet i com s'ha fet. Així mateix, totes les decisions i conclusions hauran de ser presentades de forma raonada i clara, **contextualitzant els resultats**, és a dir, especificant tots i cadascun dels passos que s'hagin dut a terme per resoldre'ls.


D'aquesta manera es demana a l'estudiant que completi els passos següents amb el job de dades preparat a la Practica 1:

**Models no supervisats**

1. Aplicar un model **no supervisat** basat en el concepte de distància, sobre el joc de dades.

2. Aplicar de nou el model anterior, però usant una **mètrica de distància diferent** i comparar-ne els resultats amb els mètodes anteriors.

3. Utilitzar els algorismes **DBSCAN i OPTICS**, provant amb diferents valors del paràmetre `eps` i `minPts`, i es comparen els resultats amb els mètodes anteriors.

**Models supervisats**

4. Seleccionar una mostra d'entrenament i una de test utilitzant les proporcions que es considerin més adequades en funció de la disponibilitat de dades. Justificar aquesta selecció.

5. Aplicar un model de generació de regles a partir d' **arbres de decisió** ajustant les diferents opcions de creació. Obtenir l'arbre sense i amb opcions de poda. Obtenir la matriu de confusió. Finalment, comparar-ne els resultats.

6. Aplicar un **model supervisat** diferent del del punt 5., s'ha de triar entre els que s'han vist al material docent de l'assignatura. Comparar el resultat amb el model generat anteriorment.

7. Identificar eventuals **limitacions** del dataset seleccionat i **analitzar els riscos** per al cas d'ús del model per a classificar una nova dada.


******
# Criteris d'avaluació
******

* Exercici 1 - 25%
  - Es genera un model no supervisat.
  - S'analitzen, mostren i comenten les mesures de qualitat del model generat.
  - Es resumeixen les conclusions principals.

* Exercici 2 - 10%
  - Es genera de nou el model no supervisat anterior, però usant una mètrica de distància diferent.
  - Es mostren i es comenten les mesures de qualitat del model generat.
  - Addicionalment, es comparen els dos models no supervisats amb mètriques de distància diferents.
  - Es resumeixen les conclusions principals.

* Exercici 3 - 10%
  - S'apliquen els algorismes DBSCAN i OPTICS de forma correcta.
  - Es proven, descriuen i interpreten els resultats amb diferents valors d'eps i minPts.
  - S'obté una mesura de com és de bo l'agrupament.
  - Es comparen els resultats obtinguts dels models dels exercicis 1 i 2.
  - Es resumeixen les conclusions principals.
  
* Exercici 4 - 10%
  - Se seleccionen les mostres d'entrenament i test.
  - Es justifiquen les proporcions seleccionades.

* Exercici 5 - 20%
  - Es generen regles i es comenten i interpreten les més significatives.
  - S'extreuen les regles del model en format text i gràfic.
  - Addicionalment, es genera matriu de confusió per mesurar la capacitat predictiva de l'algoritme, tenint en compte les diferents mètriques associades a aquesta matriu (precisió, sensibilitat, especificitat...).
  - Es comparen i interpreten els resultats (sense i amb opcions de poda), explicant els avantatges i els inconvenients del model generat respecte a un altre mètode de construcció.
  - S'avalua la taxa d'error de l'arbre generat, l'eficiència a la classificació (a les mostres d'entrenament i test) i la comprensibilitat del resultat.
  - Es resumeixen les conclusions principals.

* Exercici 6 - 15%
  - Es prova amb una variació o un altre enfocament algorítmic.
  - Es detalla, comenta i avalua la qualitat de classificació.
  - Es comparen i es comenten els resultats de manera exhaustiva amb el mètode de construcció anterior.

* Exercici 7 - 10%
  - S'identifica quines possibles limitacions tenen les dades que has seleccionat per obtenir conclusions amb els models (supervisat i no supervisat)
  - S'identifiquen possibles riscos de fer servir el model (mínim 300 paraules).
  

******
# Recursos de programació
******
* Incloem en aquest apartat una llista de recursos de programació per a mineria de dades on podreu trobar exemples, idees i inspiració:
 
  + [Espai de recursos UOC per a ciència de dades](http://datascience.recursos.uoc.edu/es/)
  + [Cercador de codi R](https://rseek.org/)
  + [Col·lecció de cheatsheets en R](https://rstudio.com/resources/cheatsheets/)
  
******
# Format i data de lliurament
******

El format de lliurament és: l'output generat en format **.html** amb nom **username_estudiant-PRA2**.

La data límit de lliurament és el 17/01/2024.


******
# RESPOSTES
******

## Exercici 1

### Es genera un model no supervisat.

```{r message= FALSE, warning=FALSE}
if (!require('cluster')) install.packages('cluster')
library(cluster)

if (!require('stats')) install.packages('stats')
library(stats)

if (!require("flexclust")) install.packages("flexclust")
library(flexclust)

if (!require('dbscan')) install.packages('dbscan'); library('dbscan')

if(!require(ggpubr)){
    install.packages('ggpubr', repos='http://cran.us.r-project.org')
    library(ggpubr)
}
if(!require(grid)){
    install.packages('grid', repos='http://cran.us.r-project.org')
    library(grid)
}
if(!require(gridExtra)){
    install.packages('gridExtra', repos='http://cran.us.r-project.org')
    library(gridExtra)
}
if(!require(C50)){
    install.packages('C50', repos='http://cran.us.r-project.org')
    library(C50)
}
```

Carregar dataframe de la pràctica anterior.
```{r}
numeric_variables <- readRDS("numeric_variables.rds")
PropertyPrices <- readRDS("PropertyPrices.rds")
```

```{r}
PropertyPrices
numeric_variables
```
```{r}
numeric_variables$price_intervals <- as.numeric(as.character(PropertyPrices$price_intervals))

numeric_variables <- na.omit(numeric_variables)
numeric_variables <- as.data.frame(numeric_variables)

numeric_variables_noPrice <- numeric_variables[, -which(names(numeric_variables) == "price")]
```

```{r}
fit2 <- kmeans(numeric_variables_noPrice, 2)
fit4 <- kmeans(numeric_variables_noPrice, 4)
fit8 <- kmeans(numeric_variables_noPrice, 8)
fit12 <- kmeans(numeric_variables_noPrice, 12)
```

```{r}
cluster2 <- fit2$cluster
cluster4 <- fit4$cluster
cluster8 <- fit8$cluster
cluster12 <- fit12$cluster
```

```{r}
plot(numeric_variables$living_area, numeric_variables$price_intervals, col = cluster2, pch = 16)
```
La primera agrupació amb K=2 ha sigut força bona i es diferencien clarament els habitatges cars dels barats. Seguim estudiant amb agrupacions amb K superior.

```{r}
plot(numeric_variables$living_area, numeric_variables$price_intervals, col = cluster4, pch = 16)
```
La agrupació amb K=4 també és força bona, ja que es diferencien clarament 4 rangs de preu encara que en aquest segon cas les agrupacions no són del tot equilibrades.

```{r}
plot(numeric_variables$living_area, numeric_variables$price_intervals, col = cluster8, pch = 16)
```
Tornem a tenir una agrupació molt bona on es diferencien els rangs de preu.

```{r}
plot(numeric_variables$living_area, numeric_variables$price_intervals, col = cluster12, pch = 16)
```
Encara que l'agrupació és bona no es diferencien els 12 rangs de preu que hi ha, es barregen les agrupacions respecte les reals. 

```{r}
wcss2 <- fit2$tot.withinss
wcss2

wcss4 <- fit4$tot.withinss
wcss4

wcss8 <- fit8$tot.withinss
wcss8

wcss12 <- fit12$tot.withinss
wcss12
```
Veiem que en cada agrupació el Within-Cluster Sum of Squares es cada cop més petit, resultat esperable ja que cada agrupació està més concentrada i menys dispersa. 


***

## Exercici 2

### Es genera de nou el model no supervisat anterior, però usant una mètrica de distància diferent.

```{r}
kmed2 <- kmeans(numeric_variables_noPrice, 2, algorithm = "Lloyd")
kmed4 <- kmeans(numeric_variables_noPrice, 4, algorithm = "Lloyd")
kmed8 <- kmeans(numeric_variables_noPrice, 8, algorithm = "Lloyd")
kmed12 <- kmeans(numeric_variables_noPrice, 12, algorithm = "Lloyd")
```


```{r}
cluster_med2 <- kmed2$cluster
cluster_med4 <- kmed4$cluster
cluster_med8 <- kmed8$cluster
cluster_med12 <- kmed12$cluster
```

```{r}
plot(numeric_variables$living_area, numeric_variables$price_intervals, col = cluster_med2, pch = 16)
```
```{r}
plot(numeric_variables$living_area, numeric_variables$price_intervals, col = cluster_med4, pch = 16)
```
```{r}
plot(numeric_variables$living_area, numeric_variables$price_intervals, col = cluster_med8, pch = 16)
```
```{r}
plot(numeric_variables$living_area, numeric_variables$price_intervals, col = cluster_med12, pch = 16)
```
```{r}
wcss2 <- kmed2$tot.withinss
wcss2

wcss4 <- kmed4$tot.withinss
wcss4

wcss8 <- kmed8$tot.withinss
wcss8

wcss12 <- kmed12$tot.withinss
wcss12
```

Els resultats de tots els clusters són molt semblants comparats amb el kmeans aplicat anteriorment, degut a que l'algoritme és molt semblant els resultats també ho són.
L'única diferencia entre Kmeans i kmedians és que un usa la mitjana per calcular els centres dels clusters i l'altre usa la mediana, però l'algoritme és igual excepte aquest aspecte mencionat.
Encara que són molt semblants podem observar amb la Within-Cluster Sum of Squares que els clusters fets amb K-medians són una mica més dispersos.
L'algoritme de K-medians és més útil quan hi ha molt valors outliners i amb distribucions no gausianes a diferència del k-means que és més útil amb distribucions gausianes.


***

## Exercici 3

### S'apliquen els algorismes DBSCAN i OPTICS de forma correcta.

```{r}
### Executem l'algoritme OPTICS
opt1 <- optics(numeric_variables_noPrice, eps = 10000, minPts = 10)
plot(opt1, data = numeric_variables_noPrice, main = "OPTICS Clustering")

### Extracció d'un clústering DBSCAN tallant l'accessibilitat en el valor eps_cl
res1 <- extractDBSCAN(opt1, eps_cl = 100000)

plot(res1)


selected_variables <- numeric_variables[, c("kitchen_surface", "living_area", "price_intervals")]

hullplot(selected_variables, res1)
```


```{r}
### Executem l'algoritme OPTICS
opt2 <- optics(numeric_variables_noPrice, eps = 1000, minPts = 10)

### Extracció d'un clústering DBSCAN tallant l'accessibilitat en el valor eps_cl
res2 <- extractDBSCAN(opt2, eps_cl = 10000000)

plot(res2) ## negre indica soroll
# Visualitza els clústers
hullplot(numeric_variables, res2)

selected_variables <- numeric_variables[, c("kitchen_surface", "living_area", "price_intervals")]

hullplot(selected_variables, res2)

```
```{r}
### Executem l'algoritme OPTICS
opt3 <- optics(numeric_variables_noPrice, eps = 10000, minPts = 5)

### Extracció d'un clústering DBSCAN tallant l'accessibilitat en el valor eps_cl
res3 <- extractDBSCAN(opt3, eps_cl = 100000)

plot(res3)

hullplot(numeric_variables, res3)

selected_variables <- numeric_variables[, c("kitchen_surface", "living_area", "price_intervals")]

hullplot(selected_variables, res3)

```

Despés de comprobar varios valors de eps i minPts hem comprobat que com major sigui la eps, millor és l'agrupació. La segona agrupació amb la epsilon més petita és clarament la pitjor de les tres i és on surgen més punts negres (soroll) encanvi les altres dos agrupacions els clusters obtinguts tenen menys soroll.

Les agrupacions obtingudes amb aquest algoritme són millors que amb K-means i K-medians aixó és degut a que optics i dbscan poden identificar clústers de formes i mides irregulars, ja que no assumeixen que els clústers siguin esfèrics o de mida similar. DBSCAN també és eficaç per identificar punts que no pertanyen a cap clúster i que podrien ser considerats soroll. K-means requereix especificar prèviament el nombre de clústers. En canvi, DBSCAN pot descobrir el nombre de clústers de manera dinàmica, basant-se en les característiques de densitat del conjunt de dades.


*** 

## Exercici 4

Separem la variable que volem predir de totes les altres
```{r}
set.seed(222)
y <- numeric_variables[,"price_intervals"] 
X <- numeric_variables[, !names(numeric_variables) %in% c("price_intervals","price")]
```

Agafem 2/3 de les mostres per entrenar i les altres per mirar la validesa del model. La raó d'utilitzar una divisió en 2/3 i 1/3 o altres proporcions és buscar un equilibri entre tenir suficients dades d'entrenament per construir un model precís i tenir dades de prova suficients per avaluar la seva eficàcia.

```{r}
split_prop <- 3 

indexes = sample(1:nrow(numeric_variables), size=floor(((split_prop-1)/split_prop)*nrow(numeric_variables)))

trainX<-X[indexes,]
trainy<-y[indexes]
testX<-X[-indexes,]
testy<-y[-indexes]
```

Mirem que després d'agafar les dades de manera aleatoria segueixin tenint les mateixes propietats estadístiques.

```{r}
summary(trainX);
summary(trainy)
summary(testX)
summary(testy)
```
Tot sembla correcte aixi que ja podem procedir a la creació del model.
*** 

## Exercici 5

```{r}
trainy = as.factor(trainy)
model <- C50::C5.0(trainX, trainy,rules=TRUE )
summary(model)
```

Al tenir tantes variables possibles, les regles de decisió són molt numeroses, hi ha fins a 241 regles de decisió i les variables més útils per les prediccions són cadastral_income amb un 97.55% d'ús, living_area amb un 88.96%, number_of_frontages amb un 62.76%, primary_energy_consumption amb un 57.97%	i yearly_theoretical_total_energy_consumption amb un 56.84%, les altres variables segueixent sent molt útils però ja no superan el 50% d'ús.
La tassa d'error és força acceptable amb un 27.2%, comptan totes les categories possibles de preu.

Les variables més usades poden canviar al tornar a executar el codi.

### S'extreuen les regles del model en format text i gràfic.

```{r}
model1 <- C50::C5.0(trainX, trainy)
plot(model1,gp = gpar(fontsize = 9.5))
```
El model al contar amb tantes desicions i tantes variables, gràficament és impossible de veure res.

Ara repetirem tot el procés pero reduirem les variables per una millor visualització del model.

```{r}
numeric_data_short <- numeric_variables[, c("price", "cadastral_income", "toilets", "living_area", "yearly_theoretical_total_energy_consumption", "primary_energy_consumption")]

X2 <- numeric_data_short[, !names(numeric_data_short) %in% c("price")]

split_prop <- 3 

indexes = sample(1:nrow(numeric_variables), size=floor(((split_prop-1)/split_prop)*nrow(numeric_variables)))

trainX2<-X2[indexes,]
trainy2<-y[indexes]
testX2<-X2[-indexes,]
testy2<-y[-indexes]
```

```{r}
summary(trainX2);
summary(trainy2)
summary(testX2)
summary(testy2)
```

```{r}
trainy2 = as.factor(trainy2)
model2 <- C50::C5.0(trainX2, trainy2,rules=TRUE )
summary(model2)
```

Al ser variables númeriques amb tants valors possibles, els arbres de desició es crean amb masses regles i encara que decrementem significativament el nombre de variables el resultat segueix sent semblant i queden models molt difícils de representar i interpretar. Després d'aquest intent fallit de millorar la visualització del model ens quedarem amb el primer model ja que obtenia millors resultats. 


### Addicionalment, es genera la matriu de confusió per a mesurar la capacitat predictiva de l'algoritme, tenint en compte les diferents mètriques associades a aquesta matriu (precisió, sensibilitat, especificitat...)..

```{r}
predicted_model <- predict( model1, testX, type="class" )
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_model == testy) / length(predicted_model)))

```

```{r}
mat_conf<-table(testy,Predicted=predicted_model)
mat_conf
```
Els resultats obtinguts no són molt bons amb un error del 38.4277 %. Això és degut a la gran quantitat de possibles rangs de preu.
Veient la matriu de confusió l'error no està tant malament com pot semblar si mirem el percentatge. L'error queda concentrat aprop de la diagonal principal(classificació correcte), i a mesura que ens allunyem de la diagonal les errades disminueixen molt.

Els resultats són esperats perquè he treballat amb tot variables númeriques. K-Means funciona millor amb dades numèriques ja que el seu càlcul de distàncies entre punts es basa en mesures de distància euclidiana. És eficaç per agrupar les dades en clústers basats en proximitat numèrica. Els arbres de decisió poden gestionar tant variables numèriques com categòriques. Són flexibles i poden adaptar-se a diferents tipus de dades. Però com en el meu cas la totalitat de les variables són númeriques els resultats han sigut millors amb algoritmes no supervisats com optics i dbscans.

***

## Exercici 6

### Es prova amb una variació o un altre enfocament algorítmic.
```{r}

# Ajustar un model de regressió lineal
model_lineal <- lm(testy2 ~ testX2$cadastral_income + testX2$toilets + testX2$living_area + testX2$yearly_theoretical_total_energy_consumption
                   + testX2$primary_energy_consumption, data = numeric_data_short)

# Mostrar resum del model
summary(model_lineal)
```

```{r}
# Fer prediccions

df_test <- data.frame(testX2, testy2)
prediccions <- predict(model_lineal, newdata = df_test)
summary(model_lineal)

```

El que he fet és crear un model de regressió lineal amb les dades reduides només agafant part de les variables.
El model de regressió lineal funciona millor amb variables númeriques però el resultat no es una variable categórica sinó una variable númerica.
Llavors el resultat obtingut és díficil de comparar amb el model d'arbres de decisió.

L'Error Estándar és aproximadament de 16.000, tenint en compte que els preus que estimem van de 50.000 a 1.000.000, un error de 16.000 és bastant bon resultat.
***

## Exercici 7

En el cas dels models no supervisats, les dades usades han donat bastant bon resultat, sobretot amb optics i scans. Però al tenir tantes agrupacions de preus (12) era difícil aconseguir molt bons resultats, el mateix ha passat amb els arbres de desició que al tenir tantes possibles prediccions eren difícils de millorar la tassa d'error.
En el cas dels arbres de desició, he utilitzat només les variables númeriques i això a generat uns arbres impossibles d'interpretar.
Pero en general les dades usades en la pràctica és un conjunt de dades molt complet.


L'ús de models per predir preus d'habitatges comporta diverses consideracions i desafiaments. La sensibilitat a les dades és crítica, ja que dades inadequades poden impactar la precisió del model. Els models de regressió lineal assumeixen relacions lineals i suposicions d'errors, i qualsevol desviació pot resultar en prediccions imprecises. El mercat immobiliari és inherentment volàtil, amb canvis econòmics, polítics o imprevistos afectant les prediccions del model.

L'overfitting o underfitting són riscos, ja que un model massa ajustat o massa simple pot ser inadequat. Les variables no mesurades podrien ser crucials, i la seva absència pot impactar negativament les prediccions. Biaixos i discriminació poden ser incrustats en models històrics, provocant prediccions injustes. Models complexos poden mancar de transparència, dificultant la interpretació i explicació. La vulnerabilitat del model a manipulacions és una preocupació, especialment en contextos en línia.

Responsabilitat legal i ètica és fonamental, ja que prediccions errònies poden tenir repercussions significatives. Aspectes de regulació i privacitat també són crítics, especialment en la gestió de dades sensibles i personals. La gestió diligent, la curació de dades, la transparència i la comunicació clara amb les parts interessades són crucials per establir confiança. En resum, l'ús de models predictius de preus d'habitatges implica equilibriar molts factors i mitigar riscos per garantir un ús eficaç i ètic en el complex entorn immobiliari.


***